nohup: 忽略输入
Dataset has size 100
Start from 0
if epoch < burn:
            sigma_y = opt.sigma_0 + 0.9
        elif epoch < epochs:
            sigma_y = opt.sigma_0 + 0.9 * (1 - (epoch-burn) / epochs) ** 3
        elif epoch == epochs:
            sigma_y = opt.sigma_0
            if tau > 0.1:
                tau = 0.1
                epsilon = 0.01   
  0%|          | 0/100 [00:00<?, ?it/s]Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /root/.local/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth
epoch 1 PSNR: 16.430252075195312 sigma_y: 1.0 tau: 1.0
epoch 2 PSNR: 18.20661735534668 sigma_y: 1.0 tau: 1.0
epoch 3 PSNR: 20.867992401123047 sigma_y: 1.0 tau: 1.0
epoch 4 PSNR: 21.562564849853516 sigma_y: 1.0 tau: 0.9025
epoch 5 PSNR: 23.469928741455078 sigma_y: 1.0 tau: 0.9025
epoch 6 PSNR: 24.950725555419922 sigma_y: 1.0 tau: 0.9025
epoch 7 PSNR: 25.917177200317383 sigma_y: 0.9129666666666666 tau: 0.9025
epoch 8 PSNR: 26.703283309936523 sigma_y: 0.8317333333333334 tau: 0.9025
epoch 9 PSNR: 26.674854278564453 sigma_y: 0.7561000000000001 tau: 0.9025
epoch 10 PSNR: 26.91025161743164 sigma_y: 0.6858666666666667 tau: 0.8145062499999999
epoch 11 PSNR: 27.47051239013672 sigma_y: 0.6208333333333335 tau: 0.8145062499999999
epoch 12 PSNR: 27.46236801147461 sigma_y: 0.5608000000000001 tau: 0.7350918906249998
epoch 13 PSNR: 27.392478942871094 sigma_y: 0.5055666666666666 tau: 0.5688000922764596
epoch 14 PSNR: 27.731918334960938 sigma_y: 0.4549333333333334 tau: 0.5688000922764596
epoch 15 PSNR: 27.869539260864258 sigma_y: 0.40869999999999995 tau: 0.5403600876626365
epoch 16 PSNR: 28.06903839111328 sigma_y: 0.3666666666666668 tau: 0.46329123015975293
epoch 17 PSNR: 28.425777435302734 sigma_y: 0.32863333333333333 tau: 0.44012666865176525
epoch 18 PSNR: 28.558086395263672 sigma_y: 0.2944 tau: 0.37735360253530714
epoch 19 PSNR: 28.769641876220703 sigma_y: 0.26376666666666665 tau: 0.3235335449737089
epoch 20 PSNR: 28.84992790222168 sigma_y: 0.23653333333333335 tau: 0.2919890243387723
epoch 21 PSNR: 28.986778259277344 sigma_y: 0.21250000000000002 tau: 0.2378268852553321
epoch 22 PSNR: 29.053401947021484 sigma_y: 0.19146666666666667 tau: 0.1937114844585008
epoch 23 PSNR: 29.273998260498047 sigma_y: 0.17323333333333335 tau: 0.1937114844585008
epoch 24 PSNR: 29.244783401489258 sigma_y: 0.15760000000000002 tau: 0.17482461472379698
epoch 25 PSNR: 29.323223114013672 sigma_y: 0.1443666666666667 tau: 0.15777921478822676
epoch 26 PSNR: 29.375564575195312 sigma_y: 0.13333333333333336 tau: 0.15777921478822676
epoch 27 PSNR: 29.39923858642578 sigma_y: 0.12430000000000002 tau: 0.1285121565651031
epoch 28 PSNR: 29.44482421875 sigma_y: 0.11706666666666668 tau: 0.1285121565651031
epoch 29 PSNR: 29.524314880371094 sigma_y: 0.11143333333333333 tau: 0.1285121565651031
epoch 30 PSNR: 29.623992919921875 sigma_y: 0.1072 tau: 0.11018311023500525
epoch 31 PSNR: 29.598770141601562 sigma_y: 0.1 tau: 0.04876749791155295
epoch 32 PSNR: 29.621978759765625 sigma_y: 0.1 tau: 0.04401266686517654
epoch 33 PSNR: 29.627613067626953 sigma_y: 0.1 tau: 0.04401266686517654
epoch 34 PSNR: 29.691579818725586 sigma_y: 0.1 tau: 0.04401266686517654
epoch 35 PSNR: 29.69768714904785 sigma_y: 0.1 tau: 0.04401266686517654
epoch 36 PSNR: 29.711074829101562 sigma_y: 0.1 tau: 0.04401266686517654
epoch 37 PSNR: 29.702884674072266 sigma_y: 0.1 tau: 0.04401266686517654
epoch 38 PSNR: 29.732511520385742 sigma_y: 0.1 tau: 0.04401266686517654
epoch 39 PSNR: 29.81947135925293 sigma_y: 0.1 tau: 0.04401266686517654
epoch 40 PSNR: 29.865646362304688 sigma_y: 0.1 tau: 0.04401266686517654
epoch 41 PSNR: 29.93813705444336 sigma_y: 0.1 tau: 0.04401266686517654
epoch 42 PSNR: 29.955974578857422 sigma_y: 0.1 tau: 0.04401266686517654
epoch 43 PSNR: 29.956390380859375 sigma_y: 0.1 tau: 0.04401266686517654
epoch 44 PSNR: 29.9857177734375 sigma_y: 0.1 tau: 0.04401266686517654
epoch 45 PSNR: 29.988183975219727 sigma_y: 0.1 tau: 0.04401266686517654
epoch 46 PSNR: 29.99936294555664 sigma_y: 0.1 tau: 0.04401266686517654
epoch 47 PSNR: 30.00368881225586 sigma_y: 0.1 tau: 0.04401266686517654
epoch 48 PSNR: 30.05394744873047 sigma_y: 0.1 tau: 0.04401266686517654
epoch 49 PSNR: 30.054126739501953 sigma_y: 0.1 tau: 0.04401266686517654
epoch 50 PSNR: 30.0372314453125 sigma_y: 0.1 tau: 0.04181203352191771
epoch 51 PSNR: 30.031356811523438 sigma_y: 0.1 tau: 0.04181203352191771
epoch 52 PSNR: 30.022552490234375 sigma_y: 0.1 tau: 0.04181203352191771
epoch 53 PSNR: 30.027122497558594 sigma_y: 0.1 tau: 0.039721431845821824
epoch 54 PSNR: 30.013629913330078 sigma_y: 0.1 tau: 0.039721431845821824
epoch 55 PSNR: 30.055601119995117 sigma_y: 0.1 tau: 0.039721431845821824
epoch 56 PSNR: 30.029008865356445 sigma_y: 0.1 tau: 0.039721431845821824
epoch 57 PSNR: 30.050304412841797 sigma_y: 0.1 tau: 0.039721431845821824
epoch 58 PSNR: 30.074522018432617 sigma_y: 0.1 tau: 0.039721431845821824
epoch 59 PSNR: 30.06536102294922 sigma_y: 0.1 tau: 0.039721431845821824
epoch 60 PSNR: 30.045555114746094 sigma_y: 0.1 tau: 0.039721431845821824
epoch 61 PSNR: 30.065929412841797 sigma_y: 0.1 tau: 0.039721431845821824
epoch 62 PSNR: 30.1351261138916 sigma_y: 0.1 tau: 0.039721431845821824
epoch 63 PSNR: 30.150968551635742 sigma_y: 0.1 tau: 0.039721431845821824
epoch 64 PSNR: 30.234678268432617 sigma_y: 0.1 tau: 0.039721431845821824
epoch 65 PSNR: 30.17462921142578 sigma_y: 0.1 tau: 0.039721431845821824
epoch 66 PSNR: 30.242889404296875 sigma_y: 0.1 tau: 0.039721431845821824
epoch 67 PSNR: 30.21542739868164 sigma_y: 0.1 tau: 0.035848592240854196
epoch 68 PSNR: 30.241872787475586 sigma_y: 0.1 tau: 0.035848592240854196
epoch 69 PSNR: 30.237937927246094 sigma_y: 0.1 tau: 0.035848592240854196
epoch 70 PSNR: 30.26959991455078 sigma_y: 0.1 tau: 0.035848592240854196
epoch 71 PSNR: 30.293060302734375 sigma_y: 0.1 tau: 0.035848592240854196
epoch 72 PSNR: 30.279582977294922 sigma_y: 0.1 tau: 0.035848592240854196
epoch 73 PSNR: 30.29730796813965 sigma_y: 0.1 tau: 0.035848592240854196
epoch 74 PSNR: 30.313711166381836 sigma_y: 0.1 tau: 0.035848592240854196
epoch 75 PSNR: 30.304487228393555 sigma_y: 0.1 tau: 0.035848592240854196
main_sampling.py:521: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  LPIPS = loss_fn_vgg(2*orig-1.0, 2*torch.tensor(x[j]).to(torch.float32).cuda()-1.0)[0,0,0,0]

PSNR:30.1099 (0.1190), SSIM:0.79993 (0.00268), LPIPS:0.23166 (0.00241):   0%|          | 0/100 [09:57<?, ?it/s]
PSNR:30.1099 (0.1190), SSIM:0.79993 (0.00268), LPIPS:0.23166 (0.00241):   1%|          | 1/100 [09:57<16:25:16, 597.13s/it]epoch 1 PSNR: 16.386423110961914 sigma_y: 1.0 tau: 0.9025
epoch 2 PSNR: 18.0048828125 sigma_y: 1.0 tau: 0.9025
epoch 3 PSNR: 19.50719451904297 sigma_y: 1.0 tau: 0.9025

PSNR:30.1099 (0.1190), SSIM:0.79993 (0.00268), LPIPS:0.23166 (0.00241):   1%|          | 1/100 [10:34<17:26:06, 634.00s/it]
Traceback (most recent call last):
  File "main_sampling.py", line 1040, in <module>
    sample_image(opt=opt, config=config, model_config=model_config, device=device)
  File "main_sampling.py", line 483, in sample_image
    xt = hmc(x, n, b, seq, seq_next, algo, opt, y_0, H_funcs, x_orig)
  File "main_sampling.py", line 838, in hmc
    loss_grad = torch.autograd.grad(loss, x_proposal, retain_graph=False)[0]
  File "/root/miniconda3/envs/NHMC/lib/python3.8/site-packages/torch/autograd/__init__.py", line 436, in grad
    result = _engine_run_backward(
  File "/root/miniconda3/envs/NHMC/lib/python3.8/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/root/miniconda3/envs/NHMC/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 67, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 783170) is killed by signal: Terminated. 
